Natural Language Processing
===========================


What can be (possibly) done with NLP?

- Answering question  
- Information Extraction (e.g. extracting calendar specific info from email)
- Sentiment Analysis (e.g. determine the overall sentiment about a product after analysing several reviews)
- Machine Translation (fully automatic or assisted)
- Spam Detection
- POS (part of speech) tagging
- Summarization

Note: Not all of these are completely doable.



Ambiguity - "Crash Blossoms" Problem
------------------------------------

`Violinist Linked to JAL Crash Blossoms` can be interpreted as –

	[Violinist Linked] to [JAL Crash Blossoms]
	[Violinist Linked to JAL Crash] [Blossoms]

Second one is the actual interpretation where `blossoms` is the verb. The violinist who was linked to JAL Crash is now blossoming.

But for an automated system, both are correct interpretations and it's difficult to determine the actual one.

Another example –

	>> Teacher Strikes Idle Kids
	[Teacher] Strikes [Idle Kids] # Teacher hits the kids
	[Teacher Strikes] [Idle Kids] # Teache Strikes make the kids idle

Other things making NLP hard –

- Non-standard English (hipster tongue)
- Segmentation Issues
- Idioms & Neologisms
- Tricky Entity Names



Basic Text Processing
---------------------

Regular Expression is the primary tool for basic text parsing and it plays surprisingly large role in NLP. Sophisticated sequences of regex are often the first model for any text processing. For many hard tasks, machine learning classifiers are used.

Suppose we want to search for all the occurence of "the" or "The" in a passage, we would try the following regex-es –

- `the` - Missed capitalized occurences
- `[Tt]he` - Incorrectly returns words like `other` and `theology`.
- `[^A-Za-z][Th]he[^A-Za-z]` correctly returns the desired result

The process we went thorugh above to refine our regex was based on fixing two kind of errors

- False Positives - Matching things that we should not have matched
- False Negatives - Not matching things that we should have matched

These two classes of errors are very frequent in NLP.

Reducing the error rate for an application often involves two antagonistic efforts:

- Increasing accuracy or precision (minimizing false positives)
- Increasing coverage or recall (minimizing false negatives)

As a practical example, [Stanford English Tokenizer](http://nlp.stanford.edu/software/tokenizer.shtml) makes use of many simple regular expressions to parse a text into tokens such as month, date, phone number, etc.


Word Tokenization
-----------------

	I do uh main- mainly business data processing

- `main-` - fragment
- `uh` - filled pause

---

	Seuss's cat in the hat is different from other cats!

- `cat` and `cats` = same lemma
- `cat` and `cats` = different wordforms

---

	they lay back on the San Francisco grass and looked at the stars and their

- 15 tokens (or 14 depending on whether we consider `San Francisco` as one)
- 13 types (or 12) (or 11?) (`the` is repeated once, `San Francisco` can be considered as a single type and `they` & `their` have different word forms but are same lemma.)


---

- **Lemma**: same stem, part of speech, rough word sense
- **Wordform**: the full inflected surface form
- **Type**: an element of the vocabulary / unique words
- **Token**: an instance of that type in running text

- **`N`** = Number of tokens
- **`V`** = Vocabulary = Set of types
	- **`|V|`** is the size of vocabulary

---

About some of popular corpuses –

| Corpus                         | Tokens = `N` | Types = `\|V\|` |
|--------------------------------|--------------|-------------|
| Switchboard Phone Conversation | 2.4mil       | 20k         |
| Shakespeare                    | 884k         | 31k         |
| Google N-grams                 | 1 trillion   | 13mil       |

---

Following command can be used to count the number of types in a text –

	tr -sc 'A-Za-z' '\n' < pg100.txt | tr 'A-Z' 'a-z' | sort | uniq -c | sort -n -r

- `tr -sc 'A-Za-z' '\n'` replaces all the non-alphabetical characters with new line character
- `tr 'A-Z' 'a-z'` converts everything to lowercase
- `sort` then just sorts the list of words each in a new line
- `uniq -c` makes a list of unique words (types) with count of occurence
- `sort -n -r` sorts the new list of types based on the count of occurence in descending order

---

Issues in Tokenization

- `Finland's capital` – `Finland`, `Finlands` or `Finland's`?
- `what're`, `I'm`, `isn't` – `What are`, `I am`, `is not`
- `Hewlett-Packard` – `Hewlett Packard`?
- `state-of-the-art` – `state of the art`?
- `Lowercase` – `lower-case`, `lowercase`, `lower case`?
- `San Francisco` – One token or two?
- `m.p.h.`, `PhD.` – ??

Over all these craziness that our parser has to deal with, other languages have their own weird rules.

**French** – `L'ensemble`

- One token or two?
- `L`, `L'` or `Le`?
- Want it to match with `un ensemble`

**German** - Noun compounds are not segmented, resulting in crazy ass 50 character words. So they need compound splitter.

**Chinese & Japanese** - No spaces between words. `Japanese` fuck up the things even more by having multiple syllabary like Hiragana, Katakana, Kanji in which characters can be mixed up to form a different character.

---

**Maximum Matching Word Segmentation Algorithm** can be used to separate out the words in chinese. It works by matching the longest word in the dictionary at the beginning of the string and then moves the cursor forward to find the next possible word and so on.

Trying it on english –

- `Thecatinhat` - `The cat in hat`
- `Thetabledownthere` - `Theta bled own there` but we wanted `The table down there`

Not a good algo for psuedo-english. But it works very well for chinese. Mordern probabilistic segmentation works even better.